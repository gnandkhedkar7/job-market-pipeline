# Job Market Data Pipeline

An end-to-end, containerized data pipeline that scrapes real-world job postings, cleans and normalizes noisy data, extracts in-demand skills, and produces daily skill-demand analytics using PostgreSQL.

This project is designed to demonstrate real-world data engineering practices: resilient ingestion, structured transformations, explicit data quality handling, and reproducible automation.

---

## ğŸš€ Features

- Scrape and store raw HTML job postings
- Parse job listings from search result pages
- Normalize job titles and explicitly handle dropped records
- Extract skills from job descriptions
- Generate daily skill-demand analytics
- Fully automated using Docker Compose
- Idempotent and restart-safe pipeline

---

## ğŸ—ï¸ Architecture Overview

```text

.
Scraper (Playwright)
        â†“
raw_job_postings
        â†“
Parser
        â†“
parsed_job_postings
        â†“
Cleaner
        â†“
clean_job_postings
        â†“
Skill Extractor
        â†“
job_skills
        â†“
daily_skill_counts

```

Each stage persists its output to PostgreSQL, enabling easy debugging, replay, and auditing.

---

## Data Source

- **Arbeitnow (public job boards for jobs in Germany)**
- **Scraped using Playwright (headless Chromium)**

---

## ğŸ§± Tech Stack

- **Python 3.12**
- **PostgreSQL 15**
- **SQLAlchemy**
- **Playwright**
- **BeautifulSoup**
- **Docker & Docker Compose**

---

## ğŸ“‚ Project Structure

```text

.
â”œâ”€â”€ db/
â”‚ â”œâ”€â”€ init.sql # Database schema (sourceof truth)
â”‚ â””â”€â”€ analytics.sql # Analytics queries
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ analytics/ # Analysis logic
â”‚ â”œâ”€â”€ db/ # Database utilities
â”‚ â”œâ”€â”€ parser/ # HTML parsing logic
â”‚ â”œâ”€â”€ cleaning/ # Normalization & skill extraction
â”‚ â”œâ”€â”€ experiments/ # Non-production experiments
â”‚ â”œâ”€â”€ scraper/ # Scraper logic
â”‚ â””â”€â”€ scripts/ # Pipeline orchestration
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env # Not committed
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ—„ï¸ Database Tables

### Core Tables

- `raw_job_postings` â€” raw HTML snapshots
- `parsed_job_postings` â€” extracted job metadata
- `clean_job_postings` â€” normalized titles with drop flags
- `job_skills` â€” exploded skills per job
- `daily_skill_counts` â€” aggregated analytics

All tables are created in **`db/init.sql`**.

---

## â–¶ï¸ How to Run the Pipeline

### 1ï¸âƒ£ Start services

```bash
docker compose up --build
```

### 2ï¸âƒ£ Run scraper manually

```bash
docker compose run pipeline python -m src.scraper.arbeitnow_scraper
```

### 3ï¸âƒ£ Run the full pipeline

```bash
docker compose run pipeline
```

The pipeline will: 1. Parse jobs 2. Clean and normalize titles 3. Extract skills 4. Update daily skill analytics

## ğŸ” Inspecting the Database

```bash
docker exec -it job_pipeline_postgres psql -U pipeline_user -d job_pipeline
```

Example checks

```bash
SELECT COUNT(*) FROM clean_job_postings;
SELECT COUNT(*) FROM job_skills;
SELECT * FROM daily_skill_counts ORDER BY date DESC, job_count DESC;
```

## ğŸ“Š Example Analytics

Top skills (latest day):

```bash
SELECT *
FROM daily_skill_counts
ORDER BY date DESC, job_count DESC
LIMIT 10;
```

## ğŸ§ª Data Quality Handling

    * Titles are normalized using rule-based logic
    * Jobs with missing or invalid titles are explicitly dropped
    * Drop reasons are stored (missing_title, title_too_short, etc.)
    * Jobs with zero extracted skills are logged

## ğŸ” Reproducibility

The database schema is fully defined in db/init.sql.

To reset everything from scratch:

```bash
docker compose down -v
docker compose up --build
```

## ğŸ“Œ Design Principles

    * Explicit data states (raw â†’ parsed â†’ clean)
    * No silent failures
    * SQL-first analytics
    * Simple, explainable logic over premature ML

## Project Status
    
    * âœ… v1 complete
    * ğŸ”œ v2: additional job boards, trend analysis, optional dashboard

## ğŸ”® Future Extensions (Optional)

    * Scheduled runs (cron / Prefect)
    * Streamlit dashboard
    * Skill trend detection
    * Role clustering using TF-IDF

## ğŸ‘¤ Author

**Gauri Nandkhedkar**  
GitHub: [@gauri-nandkhedkar](https://github.com/gauri-nandkhedkar)  
LinkedIn: [@gauri-nandkhedkar](https://www.linkedin.com/in/gauri-nandkhedkar/)
